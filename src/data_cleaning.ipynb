{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punc = string.punctuation[:2] + string.punctuation[3:]\n",
    "print(punc)\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting normalise\n",
      "  Downloading normalise-0.1.8-py3-none-any.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 1.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/karan/.local/lib/python3.8/site-packages (from normalise) (1.7.1)\n",
      "Requirement already satisfied: numpy in /home/karan/.local/lib/python3.8/site-packages (from normalise) (1.21.2)\n",
      "Requirement already satisfied: scikit-learn in /home/karan/.local/lib/python3.8/site-packages (from normalise) (1.0)\n",
      "Collecting roman\n",
      "  Downloading roman-3.3-py2.py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: nltk in /home/karan/.local/lib/python3.8/site-packages (from normalise) (3.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/karan/.local/lib/python3.8/site-packages (from scikit-learn->normalise) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/karan/.local/lib/python3.8/site-packages (from scikit-learn->normalise) (1.0.1)\n",
      "Requirement already satisfied: regex in /home/karan/.local/lib/python3.8/site-packages (from nltk->normalise) (2021.8.28)\n",
      "Requirement already satisfied: tqdm in /home/karan/.local/lib/python3.8/site-packages (from nltk->normalise) (4.62.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->normalise) (7.0)\n",
      "Installing collected packages: roman, normalise\n",
      "Successfully installed normalise-0.1.8 roman-3.3\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.semi_supervised.label_propagation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_89213/1227656324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalisation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalise\u001b[0m  \u001b[0;31m# imports the normalisation module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/normalise/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalisation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_NSWs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/normalise/normalisation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtagify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_digbased\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretagify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_ALPHA\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_clfALPHA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_NUMB\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_clfNUMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_MISC\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtag_MISC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/normalise/class_ALPHA.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmod_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtagify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_digbased\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macr_pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_NUMB\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretagify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasurements\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeas_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeas_dict_pl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/normalise/class_NUMB.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/data/clf_NUMB.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mclf_NUMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.semi_supervised.label_propagation'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "from normalise.normalisation import normalise  # imports the normalisation module\n",
    "import re\n",
    "\n",
    "languages = ['us','es']\n",
    "# to load the language specific train data\n",
    "for language in languages:\n",
    "   file = open(os.getcwd() + '/' + language + '_train.text', 'r')\n",
    "   output_file = open(os.getcwd() + '/' + language + '_train_normal.text', 'w')\n",
    "# string manipulation operations\n",
    "   for line in file:\n",
    "      line = re.sub(r'[\\s]+\\)', ')', re.sub(r'\\([\\s]+', '(', line)) # regex to remove a whitespace before and after a paranthesis\n",
    "      line = re.sub(r'[\\s]+\\]', ']', re.sub(r'\\[[\\s]+', '[', line)) # regex to remove a whitespace before and after a square bracket\n",
    "      line = re.sub(r'[\\s]+\\}', '}', re.sub(r'\\{[\\s]+', '{', line)) # regex to remove a whitespace before and after a flower bracket\n",
    "      line = re.sub(r'(?i)\\b((?:https?:\\/\\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', line)   # regex to remove the urls and twitter links\n",
    "      line = re.sub(r'-', '', line) # regex to remove '-'\n",
    "      line = re.sub(r'(.)\\1{9,}', '', line) #regex to remove repetitive characters\n",
    "      z = normalise(line, verbose=False, variety=\"Ame\")  # calling the normalise function, verbose mode: prints out complete run of program, variety: to specify as american english\n",
    "      print(z, file=output_file) # writes the data to an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def twitter_preprocess():\n",
    "    preprocessor = TextPreProcessor(\n",
    "        normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "                   'time',\n",
    "                   'date', 'number'],\n",
    "        annotate={\"hashtag\", \"elongated\", \"allcaps\", \"repeated\", 'emphasis',\n",
    "                  'censored'},\n",
    "        all_caps_tag=\"wrap\",\n",
    "        fix_text=True,\n",
    "        segmenter=\"twitter_2018\",\n",
    "        corrector=\"twitter_2018\",\n",
    "        unpack_hashtags=True,\n",
    "        unpack_contractions=True,\n",
    "        spell_correct_elong=False,\n",
    "        tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "        dicts=[emoticons]\n",
    "    ).pre_process_doc\n",
    "\n",
    "    def preprocess(name, dataset):\n",
    "        desc = \"PreProcessing dataset {}...\".format(name)\n",
    "\n",
    "        data = [preprocessor(x)\n",
    "                for x in tqdm(dataset, desc=desc)]\n",
    "        return data\n",
    "\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "def vectorize(sequence, el2idx, max_length, unk_policy=\"random\",\n",
    "              spell_corrector=None):\n",
    "    \"\"\"\n",
    "    Covert array of tokens, to array of ids, with a fixed length\n",
    "    and zero padding at the end\n",
    "    Args:\n",
    "        sequence (): a list of elements\n",
    "        el2idx (): dictionary of word to ids\n",
    "        max_length ():\n",
    "        unk_policy (): how to handle OOV words\n",
    "        spell_corrector (): if unk_policy = 'correct' then pass a callable\n",
    "            which will try to apply spell correction to the OOV token\n",
    "\n",
    "\n",
    "    Returns: list of ids with zero padding at the end\n",
    "\n",
    "    \"\"\"\n",
    "    words = numpy.zeros(max_length).astype(int)\n",
    "\n",
    "    # trim tokens after max length\n",
    "    sequence = sequence[:max_length]\n",
    "\n",
    "    for i, token in enumerate(sequence):\n",
    "        if token in el2idx:\n",
    "            words[i] = el2idx[token]\n",
    "        else:\n",
    "            if unk_policy == \"random\":\n",
    "                words[i] = el2idx[\"<unk>\"]\n",
    "            elif unk_policy == \"zero\":\n",
    "                words[i] = 0\n",
    "            elif unk_policy == \"correct\":\n",
    "                corrected = spell_corrector(token)\n",
    "                if corrected in el2idx:\n",
    "                    words[i] = el2idx[corrected]\n",
    "                else:\n",
    "                    words[i] = el2idx[\"<unk>\"]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
