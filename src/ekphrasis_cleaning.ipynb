{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def twitter_preprocess():\n",
    "    preprocessor = TextPreProcessor(\n",
    "        normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "                   'time',\n",
    "                   'date', 'number'],\n",
    "        annotate={\"hashtag\", \"elongated\", \"allcaps\", \"repeated\", 'emphasis',\n",
    "                  'censored'},\n",
    "        all_caps_tag=\"wrap\",\n",
    "        fix_text=True,\n",
    "        segmenter=\"twitter_2018\",\n",
    "        corrector=\"twitter_2018\",\n",
    "        unpack_hashtags=True,\n",
    "        unpack_contractions=True,\n",
    "        spell_correct_elong=False,\n",
    "        tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "        dicts=[emoticons]\n",
    "    ).pre_process_doc\n",
    "\n",
    "    def preprocess(name, dataset):\n",
    "        desc = \"PreProcessing dataset {}...\".format(name)\n",
    "\n",
    "        data = [preprocessor(x)\n",
    "                for x in tqdm(dataset, desc=desc)]\n",
    "        return data\n",
    "\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "def vectorize(sequence, el2idx, max_length, unk_policy=\"random\",\n",
    "              spell_corrector=None):\n",
    "    \"\"\"\n",
    "    Covert array of tokens, to array of ids, with a fixed length\n",
    "    and zero padding at the end\n",
    "    Args:\n",
    "        sequence (): a list of elements\n",
    "        el2idx (): dictionary of word to ids\n",
    "        max_length ():\n",
    "        unk_policy (): how to handle OOV words\n",
    "        spell_corrector (): if unk_policy = 'correct' then pass a callable\n",
    "            which will try to apply spell correction to the OOV token\n",
    "\n",
    "\n",
    "    Returns: list of ids with zero padding at the end\n",
    "\n",
    "    \"\"\"\n",
    "    words = numpy.zeros(max_length).astype(int)\n",
    "\n",
    "    # trim tokens after max length\n",
    "    sequence = sequence[:max_length]\n",
    "\n",
    "    for i, token in enumerate(sequence):\n",
    "        if token in el2idx:\n",
    "            words[i] = el2idx[token]\n",
    "        else:\n",
    "            if unk_policy == \"random\":\n",
    "                words[i] = el2idx[\"<unk>\"]\n",
    "            elif unk_policy == \"zero\":\n",
    "                words[i] = 0\n",
    "            elif unk_policy == \"correct\":\n",
    "                corrected = spell_corrector(token)\n",
    "                if corrected in el2idx:\n",
    "                    words[i] = el2idx[corrected]\n",
    "                else:\n",
    "                    words[i] = el2idx[\"<unk>\"]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tweets = (open(\"./../Dataset/trial/us_trial.text\", \"r\").readlines())\n",
    "emoji = (open(\"./../Dataset/trial/us_trial.labels\", \"r\").readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "preprocess = twitter_preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset hello...: 100%|██████████| 50000/50000 [00:50<00:00, 988.12it/s]\n"
     ]
    }
   ],
   "source": [
    "data = preprocess(\"hello\",tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glam',\n",
       " 'on',\n",
       " '<user>',\n",
       " 'yesterday',\n",
       " 'for',\n",
       " '<hashtag>',\n",
       " 'kcon',\n",
       " '</hashtag>',\n",
       " 'makeup',\n",
       " 'using',\n",
       " '<user>',\n",
       " 'in',\n",
       " '<hashtag>',\n",
       " 'feather',\n",
       " 'ette',\n",
       " '</hashtag>',\n",
       " ',',\n",
       " '…']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glam on @user yesterday for #kcon makeup using @user in #featherette,…\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
